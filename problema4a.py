# -*- coding: utf-8 -*-
"""problema4a.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GvWo6SYbVVuLe1TYC0zPJKxmcmuUKjbR
"""

import numpy as np
import matplotlib.pyplot as plt

def Funcion4A(x, y):
    return (x - 2)**2 + (y + 1)**2

def grad_4A(x, y):
    return np.array([2*(x - 2), 2*(y + 1)])

def gradiente_descendente(alpha, x0, y0, tol=1e-6, max_iter=100):
    x_k, y_k = x0, y0
    historia = [(x_k, y_k)]
    for i in range(max_iter):
        grad = grad_4A(x_k, y_k)
        x_k1 = x_k - alpha * grad[0]
        y_k1 = y_k - alpha * grad[1]
        historia.append((x_k1, y_k1))
        if np.linalg.norm([x_k1 - x_k, y_k1 - y_k]) < tol:
            break
        x_k, y_k = x_k1, y_k1
    return np.array(historia)

def main4A():

    x0, y0 = 0.0, 0.0
    alphas = [0.15, 0.30, 0.45, 0.60, 0.75, 0.90]

    true_min = np.array([2, -1])
    hist_dict = {}
    errors_dict = {}

    for alpha in alphas:
        hist = gradiente_descendente(alpha, x0, y0)
        errors = np.linalg.norm(hist - true_min, axis=1)
        hist_dict[alpha] = hist
        errors_dict[alpha] = errors

    all_x = np.concatenate([hist[:,0] for hist in hist_dict.values()])
    all_y = np.concatenate([hist[:,1] for hist in hist_dict.values()])
    x_min, x_max = -5.0, 5.0
    y_min, y_max = -5.0, 5.0
    all_errors = np.concatenate(list(errors_dict.values()))
    err_min, err_max = all_errors.min(), all_errors.max()
    max_iter_global = max(len(hist) for hist in hist_dict.values())

    xv, yv = np.meshgrid(np.linspace(x_min, x_max, 100),
                         np.linspace(y_min, y_max, 100))
    Lv = Funcion4A(xv, yv)

    fig, axs = plt.subplots(4, len(alphas), figsize=(4*len(alphas), 9))
    fig.suptitle("Evolución de x, y y error por iteración (Gradiente Descendente)", fontsize=14)

    for i, alpha in enumerate(alphas):
        hist = hist_dict[alpha]
        errors = errors_dict[alpha]
        iters = np.arange(len(hist))

        # Evolución de x
        axs[0, i].plot(iters, hist[:, 0], 'b-o')
        axs[0, i].set_title(f"α = {alpha} — Evolución de x")
        axs[0, i].set_xlabel("Iteraciones")
        axs[0, i].grid(True)
        axs[0, i].set_ylim(x_min, x_max)
        axs[0, i].set_xlim(0, max_iter_global)

        # Evolución de y
        axs[1, i].plot(iters, hist[:, 1], 'r-s')
        axs[1, i].set_title(f"α = {alpha} — Evolución de y")
        axs[1, i].set_xlabel("Iteraciones")
        axs[1, i].grid(True)
        axs[1, i].set_ylim(y_min, y_max)
        axs[1, i].set_xlim(0, max_iter_global)

        # Evolución del error
        axs[2, i].plot(iters, errors, 'g-o')
        axs[2, i].set_title(f"α = {alpha} — Error vs iteraciones")
        axs[2, i].set_xlabel("Iteraciones")
        axs[2, i].grid(True)
        axs[2, i].set_ylim(err_min, err_max)
        axs[2, i].set_xlim(0, max_iter_global)

        # --- Fila 4: contorno de L(x, y) ---
        cont = axs[3, i].contourf(xv, yv, Lv, levels=30, cmap='viridis_r', alpha=0.8)
        axs[3, i].contour(xv, yv, Lv, colors='k', levels=15, linewidths=0.5)
        axs[3, i].plot(hist[:, 0], hist[:, 1], 'w.-', label='Trayectoria')
        axs[3, i].scatter(2, -1, color='red', marker='*', s=100, label='Mínimo real')
        axs[3, i].set_xlim(x_min, x_max)
        axs[3, i].set_ylim(y_min, y_max)
        axs[3, i].set_xlabel("x")
        axs[3, i].set_title(f"Trayectoria sobre L(x, y)")
        axs[3, i].grid(True)
        axs[3, i].legend(loc='upper right', fontsize=8)

    axs[0, 0].set_ylabel("x", fontsize=12)
    axs[1, 0].set_ylabel("y", fontsize=12)
    axs[2, 0].set_ylabel("Error", fontsize=12)
    axs[3, 0].set_ylabel("Función L(x, y)", fontsize=12)

    plt.tight_layout(rect=[0, 0, 1, 0.96])
    plt.show()

    # ======================
    # Comparación del error global
    # ======================
    plt.figure(figsize=(8, 5))
    for alpha in alphas:
        plt.plot(errors_dict[alpha], marker='o', label=f"α = {alpha}")
    plt.title("Comparación del error para diferentes valores de α")
    plt.xlabel("Iteraciones")
    plt.ylabel("Error (distancia al mínimo)")
    plt.legend()
    plt.grid(True)
    plt.ylim(err_min, err_max)
    plt.xlim(0, max_iter_global)
    plt.show()

if __name__ == '__main__':
  main4A()

"""### Análisis de sensibilidad del valor α

Las gráficas muestran que el valor de α influye directamente en la velocidad y estabilidad del gradiente descendente.  
Con α = 0.15 la convergencia es lenta pero estable; con α = 0.3 y α = 0.45 el descenso es más rápido y suave, alcanzando el mínimo en pocas iteraciones.  
A partir de α = 0.6 surgen pequeñas oscilaciones y con α = 0.9 el método se vuelve inestable, con trayectorias que rodean el mínimo varias veces antes de estabilizarse.  
En conclusión, el método es sensible a α, y el rango óptimo según las gráficas está entre α = 0.3 y α = 0.45, donde se logra el mejor equilibrio entre rapidez y estabilidad.

### Estrategia para hallar el valor óptimo de α

Primero se prueban varios valores en escala logarítmica (por ejemplo, 0.01, 0.1, 0.3, 0.5, 0.9) observando la estabilidad y velocidad de convergencia.  
Los valores muy altos generan oscilaciones o divergencia, mientras que los muy bajos hacen el proceso lento.  
Después, se afina la búsqueda dentro del rango estable (por ejemplo, entre 0.3 y 0.5) y se elige el α que minimice el error en menos iteraciones sin perder estabilidad.
"""