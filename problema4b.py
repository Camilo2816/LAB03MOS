# -*- coding: utf-8 -*-
"""problema4b.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/170f-uMo3fVz-QKNyo5go1sJQF4JtVNg4
"""

import numpy as np
import matplotlib.pyplot as plt

# ======================
# Definición de f, gradiente y Hessiana
# ======================
def f(x, y):
    try:
        val = (x - 2)**2 * (y + 2)**2 + (x + 1)**2 + (y - 1)**2
        return np.clip(val, -1e10, 1e10)  # evitar overflow en f
    except OverflowError:
        return np.inf

def grad_f(x, y):
    try:
        df_dx = 2*(x - 2)*(y + 2)**2 + 2*(x + 1)
        df_dy = 2*(y + 2)*(x - 2)**2 + 2*(y - 1)
        grad = np.array([df_dx, df_dy])
        grad = np.clip(grad, -1e6, 1e6)  # limitar magnitud del gradiente
        return grad
    except OverflowError:
        return np.array([np.inf, np.inf])

def hess_f(x, y):
    try:
        d2f_dx2 = 2*(y + 2)**2 + 2
        d2f_dy2 = 2*(x - 2)**2 + 2
        d2f_dxdy = 4*(x - 2)*(y + 2)
        hess = np.array([[d2f_dx2, d2f_dxdy],
                         [d2f_dxdy, d2f_dy2]])
        hess = np.clip(hess, -1e6, 1e6)
        return hess
    except OverflowError:
        return np.full((2, 2), np.inf)

# ======================
# Métodos
# ======================
def gradiente_descendente(alpha, x0, y0, tol=1e-6, max_iter=100):
    x, y = x0, y0
    historia = [(x, y)]
    for i in range(max_iter):
        grad = grad_f(x, y)
        if not np.all(np.isfinite(grad)):
            print(f"[GD] Iteración {i}: gradiente no finito → {grad}")
            break
        x_new = x - alpha * grad[0]
        y_new = y - alpha * grad[1]

        # prevenir explosión
        if abs(x_new) > 1e6 or abs(y_new) > 1e6:
            print(f"[GD] Divergencia en iter {i}: ({x_new:.2e}, {y_new:.2e})")
            break

        historia.append((x_new, y_new))
        if np.linalg.norm([x_new - x, y_new - y]) < tol:
            break
        x, y = x_new, y_new
    return np.array(historia)

def newton_raphson(alpha, x0, y0, tol=1e-6, max_iter=100):
    x, y = x0, y0
    historia = [(x, y)]
    for i in range(max_iter):
        grad = grad_f(x, y)
        hess = hess_f(x, y)
        if not np.all(np.isfinite(grad)) or not np.all(np.isfinite(hess)):
            print(f"[NR] Iteración {i}: valores no finitos")
            break
        try:
            delta = np.linalg.solve(hess, grad)
        except np.linalg.LinAlgError:
            print(f"[NR] Hessiana singular en iter {i}")
            break
        x_new = x - alpha * delta[0]
        y_new = y - alpha * delta[1]

        if abs(x_new) > 1e6 or abs(y_new) > 1e6:
            print(f"[NR] Divergencia en iter {i}: ({x_new:.2e}, {y_new:.2e})")
            break

        historia.append((x_new, y_new))
        if np.linalg.norm([x_new - x, y_new - y]) < tol:
            break
        x, y = x_new, y_new
    return np.array(historia)

def main4B():
    x0, y0 = -2.0, -3.0
    alphas = [0.01, 0.05, 0.1, 0.15, 0.2, 0.7]
    tol, max_iter = 1e-6, 100

    def run(method, alpha):
        hist = method(alpha, x0, y0, tol, max_iter)
        hist = hist[np.all(np.isfinite(hist), axis=1)]  # eliminar filas no finitas
        return hist

    gd_hist, nr_hist, gd_err, nr_err = {}, {}, {}, {}

    x_min, x_max = -5.0, 5.0
    y_min, y_max = -5.0, 5.0

    xv, yv = np.meshgrid(np.linspace(x_min, x_max, 300),
                         np.linspace(y_min, y_max, 300))
    fv = f(xv, yv)

    min_idx = np.unravel_index(np.argmin(fv), fv.shape)
    min_ref = [xv[min_idx], yv[min_idx]]

    print(f"Punto mínimo aproximado en el grid:")
    print(f"x_min = {min_ref[0]:.4f}, y_min = {min_ref[1]:.4f}")

    for alpha in alphas:
        h_gd = run(gradiente_descendente, alpha)
        h_nr = run(newton_raphson, alpha)
        gd_hist[alpha] = h_gd
        nr_hist[alpha] = h_nr
        gd_err[alpha] = np.linalg.norm(h_gd - min_ref, axis=1)
        nr_err[alpha] = np.linalg.norm(h_nr - min_ref, axis=1)

    n_alpha = len(alphas)
    fig, axs = plt.subplots(4, n_alpha, figsize=(5 * n_alpha, 16))
    fig.suptitle("Comparación de Gradiente Descendente y Newton-Raphson", fontsize=16)

    for j, alpha in enumerate(alphas):
        # --- 1️⃣ Evolución de x ---
        axs[0, j].plot(gd_hist[alpha][:, 0], 'b-o', label="GD", ms=3)
        axs[0, j].plot(nr_hist[alpha][:, 0], 'r--s', label="NR", ms=3)
        axs[0, j].axhline(min_ref[0], color='m', linestyle=':', label="x óptimo")
        axs[0, j].set_title(f"α = {alpha}")
        axs[0, j].set_xlabel("Iteración")
        axs[0, j].set_ylabel("x")
        axs[0, j].grid(True)
        axs[0, j].legend(loc="best", fontsize=8)

        # --- 2️⃣ Evolución de y ---
        axs[1, j].plot(gd_hist[alpha][:, 1], 'b-o', label="GD", ms=3)
        axs[1, j].plot(nr_hist[alpha][:, 1], 'r--s', label="NR", ms=3)
        axs[1, j].axhline(min_ref[1], color='m', linestyle=':', label="y óptimo")
        axs[1, j].set_xlabel("Iteración")
        axs[1, j].set_ylabel("y")
        axs[1, j].grid(True)
        axs[1, j].legend(loc="best", fontsize=8)

        # --- 3️⃣ Contornos y trayectorias ---
        cs = axs[2, j].contourf(xv, yv, fv, levels=40, cmap="viridis_r")
        axs[2, j].contour(xv, yv, fv, levels=20, colors='k', linewidths=0.3, alpha=0.5)

        axs[2, j].plot(gd_hist[alpha][:, 0], gd_hist[alpha][:, 1],
                       'b-o', ms=3, label="GD")
        axs[2, j].plot(nr_hist[alpha][:, 0], nr_hist[alpha][:, 1],
                       'r--s', ms=3, label="NR")

        axs[2, j].plot(min_ref[0], min_ref[1], 'ko', ms=8, label="Mínimo óptimo")
        axs[2, j].set_xlim(xv.min(), xv.max())
        axs[2, j].set_ylim(yv.min(), yv.max())
        axs[2, j].set_xlabel("x")
        axs[2, j].set_ylabel("y")
        axs[2, j].grid(True, linestyle='--', alpha=0.5)
        axs[2, j].legend(loc="best", fontsize=8)

        # --- 4️⃣ Comparación de errores ---
        n_iter = min(len(gd_err[alpha]), len(nr_err[alpha]))
        axs[3, j].semilogy(np.arange(n_iter), gd_err[alpha][:n_iter],
                           'b-o', ms=3, label="GD")
        axs[3, j].semilogy(np.arange(n_iter), nr_err[alpha][:n_iter],
                           'r--s', ms=3, label="NR")
        axs[3, j].set_xlabel("Iteración")
        axs[3, j].set_ylabel("Error (log)")
        axs[3, j].grid(True, which="both", linestyle="--", alpha=0.6)
        axs[3, j].legend(loc="best", fontsize=8)

    # Barra de color común solo una vez
    fig.colorbar(cs, ax=axs[2, :], orientation='vertical', fraction=0.02, pad=0.04, label='f(x, y)')

    plt.tight_layout(rect=[0, 0, 1, 0.96])
    plt.show()

if __name__ == '__main__':
   main4B()

"""### Análisis de la sensibilidad del valor de α y estrategia para su valor óptimo

La nueva gráfica muestra que el gradiente descendente (GD) es muy sensible al valor de α.  
Con α = 0.01, converge lentamente; con α = 0.05 y 0.1, el descenso es estable y rápido.  
En cambio, para α ≥ 0.15, aparecen oscilaciones y pérdida de convergencia.  
Esto evidencia que un α demasiado grande provoca saltos sobre el mínimo, mientras que uno muy pequeño ralentiza el proceso.

El método de Newton-Raphson (NR) mantiene estabilidad en todos los casos, al depender del Hessiano y no del tamaño fijo del paso.

### Conclusión resumida

Para esta función, Newton-Raphson es el método más adecuado: converge más rápido, con mayor precisión y es menos sensible al valor de α.  
El Gradiente Descendente requiere ajustar bien α y puede divergir con pasos grandes.

**Usar Newton-Raphson cuando:**
- La función es suave y de pocas variables.  
- Se busca alta precisión y se puede asumir mayor costo por iteración.  

**Usar Gradiente Descendente cuando:**
- El problema es de gran dimensión o con datos ruidosos.  
- Se necesita bajo costo por iteración y estabilidad.  

**En resumen:** Newton-Raphson es ideal para problemas pequeños y precisos; Gradiente Descendente, para problemas grandes o de aprendizaje automático.

### Tabla comparativa de métodos

| Criterio                          | Gradiente Descendente (GD)                                    | Newton-Raphson (NR)                                        |
|----------------------------------|----------------------------------------------------------------|-------------------------------------------------------------|
| **Convergencia**                 | Lenta; depende fuertemente del valor de α                      | Muy rápida; converge en pocas iteraciones                   |
| **Estabilidad**                  | Inestable con α grandes; sensible al paso                      | Estable en todo el rango de α observado                     |
| **Precisión final**              | Aceptable solo con α óptimo                                    | Alta precisión, alcanza el mínimo con bajo error             |
| **Costo por iteración**          | Bajo (solo usa gradientes)                                     | Alto (requiere calcular e invertir el Hessiano)             |
| **Robustez frente a α**          | Muy sensible; pequeños cambios afectan el resultado            | Poco sensible; mantiene convergencia estable                |
| **Tiempo total de ejecución**    | Mayor por necesitar muchas iteraciones                         | Menor, aunque cada iteración es más costosa                 |
| **Adecuado para**                | Problemas grandes o de alto costo computacional                | Problemas pequeños y suaves donde se requiere alta precisión |

**Conclusión:**  
En este problema, Newton-Raphson supera claramente al Gradiente Descendente en velocidad, precisión y estabilidad, siendo el método más eficiente.  
Sin embargo, GD sigue siendo útil para problemas de gran escala donde el cálculo del Hessiano no es viable.
"""